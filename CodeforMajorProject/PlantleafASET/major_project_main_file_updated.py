# -*- coding: utf-8 -*-
"""Major Project - Main File updated.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hGGTLnUbZiiAUtYJKK__U0NNReiSvOaw
"""

# MAJOR PROJECT IMPLEMENTATION

import numpy as np 
import cv2
import matplotlib.pyplot as plt
from pathlib import Path

import numpy as np 
import cv2
import matplotlib.pyplot as plt
from pathlib import Path
# Filtering using Median Filter - Healthy samples
folder_dir = 'Healthy' #
save_dir = 'FilteredHealthy'

images = Path(folder_dir).glob('*.JPG')

i = 1

## Median filtering Using opencv
for img in images:
    path = str(img)
    # cv2
    org_img = cv2.imread(path)
    org_img = cv2.cvtColor(org_img, cv2.COLOR_BGR2RGB)
    med_img = cv2.medianBlur(org_img, 3)
    
    img_name = "Image" + str(i) + ".JPG"
    save_path = save_dir + '/' + img_name
    cv2.imwrite(save_path, med_img)
    i = i + 1
    print(img_name, " processed.")

# Filtering using Median Filter - Unhealthy samples
folder_dir = 'UnHealthy'
save_dir = 'FilteredUnhealthy'

images = Path(folder_dir).glob('*.JPG')

i = 1

for img in images:
    path = str(img)
    org_img = cv2.imread(path)
    org_img = cv2.cvtColor(org_img, cv2.COLOR_BGR2RGB)
    
    med_img = cv2.medianBlur(org_img, 3)
    
    img_name = "Image" + str(i) + ".JPG"
    save_path = save_dir + '/' + img_name
    cv2.imwrite(save_path, med_img)
    i = i + 1
    print(img_name, " processed.")

# Controlled Watershed Segmentation - Healthy Samples

folder_dir = 'FilteredHealthy'
save_dir = 'SegmentedHealthy'

images = Path(folder_dir).glob('*.JPG')

i = 1

for img in images:
    path = str(img)
    med_img = cv2.imread(path)
    
    gray = cv2.cvtColor(med_img,cv2.COLOR_BGR2GRAY)
    ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    
    # Step 2 [noise removal] Kernel size is 2 by 2 todo
    kernel = np.ones((2,2),np.uint8)
    # No of iterations 2 todo
    closing = cv2.morphologyEx(thresh,cv2.MORPH_CLOSE,kernel, iterations = 2)
    
    # Step 3 [sure background area]
    sure_bg = cv2.dilate(closing,kernel,iterations=3)
    
    # Step 4 [Finding sure foreground area]
    dist_transform = cv2.distanceTransform(sure_bg,cv2.DIST_L2,3)
    
    # Step 5 [Threshold]
    ret, sure_fg = cv2.threshold(dist_transform,0.1*dist_transform.max(),255,0)
    
    # Step 6 [Finding unknown region]
    sure_fg = np.uint8(sure_fg)
    unknown = cv2.subtract(sure_bg,sure_fg)
    
    # Step 7 [Initializing Markers]
    ret, markers = cv2.connectedComponents(sure_fg)
    # Add one to all labels so that sure background is not 0, but 1
    markers = markers+1
    # Now, mark the region of unknown with zero
    markers[unknown==255] = 0
    
    # Step 8 [Applying Water-Shed]
    markers = cv2.watershed(med_img,markers)
    med_img[markers == -1] = [255,0,0]
    
    img_name = "Seg_Image" + str(i) + ".JPG"
    save_path = save_dir + '/' + img_name
    cv2.imwrite(save_path, med_img)
    i = i + 1
    print(img_name, " processed.")

# Controlled Watershed Segmentation - Unhealthy Samples
folder_dir = 'FilteredUnhealthy'
save_dir = 'SegmentedUnhealthy'

images = Path(folder_dir).glob('*.JPG')

i = 1

for img in images:
    path = str(img)
    med_img = cv2.imread(path)
    
    gray = cv2.cvtColor(med_img,cv2.COLOR_BGR2GRAY)
    ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    
    # Step 2 [noise removal]
    kernel = np.ones((2,2),np.uint8)
    closing = cv2.morphologyEx(thresh,cv2.MORPH_CLOSE,kernel, iterations = 2)
    
    # Step 3 [sure background area]
    sure_bg = cv2.dilate(closing,kernel,iterations=3)
    
    # Step 4 [Finding sure foreground area]
    dist_transform = cv2.distanceTransform(sure_bg,cv2.DIST_L2,3)
    
    # Step 5 [Threshold]
    ret, sure_fg = cv2.threshold(dist_transform,0.1*dist_transform.max(),255,0)
    
    # Step 6 [Finding unknown region]
    sure_fg = np.uint8(sure_fg)
    unknown = cv2.subtract(sure_bg,sure_fg)
    
    # Step 7 [Initializing Markers]
    ret, markers = cv2.connectedComponents(sure_fg)
    # Add one to all labels so that sure background is not 0, but 1
    markers = markers+1
    # Now, mark the region of unknown with zero
    markers[unknown==255] = 0
    
    # Step 8 [Applying Water-Shed]
    markers = cv2.watershed(med_img,markers)
    med_img[markers == -1] = [255,0,0]
    
    img_name = "Seg_Image" + str(i) + ".JPG"
    save_path = save_dir + '/' + img_name
    cv2.imwrite(save_path, med_img)
    i = i + 1
    print(img_name, " processed.")

# pip install glrlm

# GLRLM Feature Extraction - Healthy  Gray Level Run Length Matrix
from glrlm import GLRLM
#import cv2
#from pathlib import Path
import pandas as pd

result = []

folder_dir = 'SegmentedHealthy'

images = Path(folder_dir).glob('*.JPG')

i = 1
for img in images:
    path = str(img)
    seg_img = cv2.imread(path)

    app = GLRLM()
    glrlm = app.get_features(seg_img, 10) 
    result.append(glrlm.Features)
        
      
print(result)

df = pd.DataFrame(result)
df.to_excel(excel_writer = "GLRLM_Healthy.xlsx")

# GLRLM Feature Extraction - Unhealthy

result = []

folder_dir = 'Segmented_Unhealthy'

images = Path(folder_dir).glob('*.JPG')

for img in images:
    path = str(img)
    seg_img = cv2.imread(path)

    app = GLRLM()
    glrlm = app.get_features(seg_img, 8)
    result.append(glrlm.Features)
        
      
df = pd.DataFrame(result)
df.to_excel(excel_writer = "GLRLM_Unhealthy.xlsx")

# GLCM Feature Extraction - Healthy
from pathlib import Path
import cv2
import os
import re

# -------------------- Utility functions ------------------------
def normalize_label(str_):
    str_ = str_.replace(" ", "")
    str_ = str_.translate(str_.maketrans("","", "()"))
    str_ = str_.split("_")
    return ''.join(str_[:2])

def normalize_desc(folder, sub_folder):
    text = folder 
    text = re.sub(r'\d+', '', text)
    text = text.replace(".", "")
    text = text.strip()
    return text

# -------------------- Load Dataset ------------------------
 
imgs = [] #list image matrix 
labels = []
descs = []

folder_dir = "Segmented_Healthy"

images = Path(folder_dir).glob('*.JPG')

for img in images:
    path = str(img)
    seg_img = cv2.imread(path)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            
    h, w = gray.shape
    ymin, ymax, xmin, xmax = h//3, h*2//3, w//3, w*2//3
    crop = gray[ymin:ymax, xmin:xmax]
    resize = cv2.resize(crop, (0,0), fx=0.5, fy=0.5)
    imgs.append(resize)
    
    labels.append(normalize_label(path.splitext(filename)[0]))
    descs.append(normalize_desc(folder))


from skimage.feature.texture import graycomatrix, graycoprops

# ----------------- calculate greycomatrix() & greycoprops() for angle 0, 45, 90, 135 ----------------------------------
def calc_glcm_all_agls(img, label, props, dists=[5], agls=[0, np.pi/4, np.pi/2, 3*np.pi/4], lvl=256, sym=True, norm=True):
    
    glcm = graycomatrix(img, 
                        distances=dists, 
                        angles=agls, 
                        levels=lvl,
                        symmetric=sym, 
                        normed=norm)
    feature = []
    glcm_props = [propery for name in props for propery in graycoprops(glcm, name)[0]]
    for item in glcm_props:
            feature.append(item)
    feature.append(label) 
    
    return feature


# ----------------- call calc_glcm_all_agls() for all properties ----------------------------------
properties = ['dissimilarity', 'correlation', 'homogeneity', 'contrast', 'ASM', 'energy']

glcm_all_agls = []
for img, label in zip(imgs, labels): 
    glcm_all_agls.append(
            calc_glcm_all_agls(img, 
                                label, 
                                props=properties)
                            )
 
columns = []
angles = ['0', '45', '90','135']
for name in properties :
    for ang in angles:
        columns.append(name + "_" + ang)
        
columns.append("label")


import pandas as pd 

# Create the pandas DataFrame for GLCM features data
glcm_df = pd.DataFrame(glcm_all_agls, 
                      columns = columns)

glcm_df.head(15)

# Whale Optimization Algorithm
# !pip install ReliefF
# !pip install -U Py_FS

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

data = pd.read_csv(r"C:\Users\hardi\Desktop\CodeforMajorProject(2)\CodeforMajorProject\PlantleafASET\Combined Features.csv") #ensure file
data.head()

# WAO Seperating Features
features = data.iloc[:, 1:]
print(features.head())

target = data.iloc[:, 0]
target.replace({'Healthy':1, 'Unhealthy':0}, inplace=True)
print(target.head())

# feature selection
from Py_FS.wrapper.nature_inspired.WOA import WOA as FS
solution = FS(num_agents=10, max_iter=100, train_data=features, train_label=target, save_conv_graph=True)

#
solution.best_agent

# convert to integer output index array
ans = solution.best_agent

index = ans.astype(int)
print(index)

# use index where value is 1 store the index number in results array
results = []
count = 0

for i in index:
    if i == 1:
        results.append(count)
    
    count = count + 1
print(results)


## corresponding to indices extract column from features array
dfW = features.iloc[: , results]
dfW['Label'] = target
print(dfW.head())
dfW.to_csv("WOA_selected.csv")

# Building ANN df dependency
X = pd.DataFrame(dfW.iloc[:, 0:7].values)
Y = dfW.iloc[:, -1].values

print(X.head())


# Split into Training & Testing data
from sklearn.model_selection import train_test_split 
# X and Y from above

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)

# Scaling data 
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.fit_transform(x_test)

print(x_train[0:4])


# define classifier and fitiing
import keras
from keras.models import Sequential
from keras.layers import Dense

classifier = Sequential()

# Adding input & first hidden layer
classifier.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'relu', input_dim = 7))

# Second hidden layer
classifier.add(Dense(5, kernel_initializer = 'uniform', activation = 'sigmoid'))

# Third hidden layer
classifier.add(Dense(5, kernel_initializer = 'uniform', activation = 'relu'))

#classifier.add(Dense(2, kernel_initializer = 'uniform', activation = 'relu'))

# Output layer
classifier.add(Dense(1, kernel_initializer = 'uniform', activation = 'sigmoid'))

# Compiling ANN & fitting data
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

classifier.fit(x_train, y_train, batch_size=10, epochs=25)

### accuracy matrix
y_pred = classifier.predict(x_test)
y_pred = (y_pred > 0.5)

from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

print("Confusion Matrix")

cm = confusion_matrix(y_test, y_pred)
print(cm)

print("Accuracy Score:", accuracy_score(y_test, y_pred))
print("Precision Score:", precision_score(y_test, y_pred))
print("Recall Score:", recall_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred))
#-------------- ANN ends here

#------------------------ BREAK

# Creating the SVM Model
from sklearn.svm import SVC
from sklearn import metrics

svc = SVC(C=1.0, random_state=1, kernel='linear')
 
# Fit the model
svc.fit(x_train, y_train)

# Make the predictions
y_predict1 = svc.predict(x_test)
 
# Measure the performance
print("Accuracy score: %.3f" %metrics.accuracy_score(y_test, y_predict1))
print("Precision:", metrics.precision_score(y_test, y_predict1))
print("Recall:", metrics.recall_score(y_test, y_predict1))
print("F1 Score:", metrics.f1_score(y_test, y_predict1))

